{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "823b2356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "installed python-dotenv\n",
      "installed transformers, torch, huggingface_hub, datasets and torchvision\n",
      "Initialised environment variables\n"
     ]
    }
   ],
   "source": [
    "# pre-requisites \n",
    "\n",
    "! pip install python-dotenv > /dev/null\n",
    "print(\"installed python-dotenv\")\n",
    "! pip install transformers torch huggingface_hub torchvision datasets> /dev/null\n",
    "print(\"installed transformers, torch, huggingface_hub, datasets and torchvision\")\n",
    "\n",
    "# Imports \n",
    "import pandas as pd \n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import load_dataset \n",
    "from huggingface_hub import login\n",
    "\n",
    "#ENV Variables \n",
    "TOKEN = os.getenv(\"TOKEN\")\n",
    "TOKENIZERS_PARALLELISM = os.getenv(\"TOKENIZERS_PARALLELISM\")\n",
    "DATASET_PATH = 'fol_dataset.csv' \n",
    "TEST_SIZE = 0.2\n",
    "SEED = 42\n",
    "\n",
    "MODEL_LIST = [\n",
    "    \"google/gemma-3-1b-it\"\n",
    "]\n",
    "login(token = TOKEN)\n",
    "\n",
    "print(\"Initialised environment variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d43546",
   "metadata": {},
   "source": [
    "Writing the predefinded functions in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8546db80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(batch):\n",
    "     inputs = tokenizer(\n",
    "         batch['expression'],\n",
    "         max_length=128,\n",
    "         truncation=True,\n",
    "         padding = 'max_length',\n",
    "         \n",
    "     )    \n",
    "    \n",
    "     labels = tokenizer(\n",
    "         batch['english_description'],\n",
    "         max_length =128,\n",
    "         truncation = True, \n",
    "         padding = 'max_length'\n",
    "     )\n",
    "     inputs['labels']=labels['input_ids']\n",
    "     return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e418f3df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9959db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the training loop in order to train the model \n",
    "\n",
    "for model in MODEL_LIST:\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model)\n",
    "    \n",
    "    dataset = load_dataset('csv', data_files = DATASET_PATH )['train']\n",
    "    split_dataset = dataset.train_test_split(test_size = TEST_SIZE, seed = SEED)\n",
    "    \n",
    "    train_set = split_dataset['train']\n",
    "    test_set = split_dataset['test']\n",
    "\n",
    "    train_tokenized = train_set.map(preprocess, batched=True)\n",
    "    test_tokenized = test_set.map(preprocess, batched=True)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        \n",
    "            output_dir = \"./results\",\n",
    "            per_device_train_batch_size = 12,\n",
    "            per_device_eval_batch_size = 12,\n",
    "            logging_steps = 2,\n",
    "            eval_steps = 2, \n",
    "            num_train_epochs = 5,\n",
    "            learning_rate = 2e-5,\n",
    "    \n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        \n",
    "            model = model,\n",
    "            args = training_args,\n",
    "            train_dataset = train_tokenized,\n",
    "            eval_dataset = test_tokenized,\n",
    "    \n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    model.save_pretrained(\"/usr3/allanp/llm_reasoning_notebook_experiments/model_ckpts\", from_pt=True) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd12b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df=pd.read_csv(\"fol_dataset_reversed.csv\")\n",
    "# column_titles=['english_description','expression']\n",
    "# df=df.reindex(columns=column_titles)\n",
    "# df.to_csv('fol_dataset.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aa94cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.6425\n",
      "8.9203\n",
      "12.8907\n",
      "6.6149\n",
      "2.9383\n",
      "2.0198\n",
      "1.7966\n",
      "1.3202\n",
      "1.2446\n",
      "1.2575\n",
      "0.9901\n",
      "4.6749\n",
      "1.3118\n",
      "0.991\n",
      "1.949\n",
      "1.1092\n",
      "0.9036\n",
      "1.0823\n",
      "0.9094\n",
      "0.8511\n",
      "1.0318\n",
      "0.8666\n",
      "0.8301\n",
      "0.9548\n",
      "0.7413\n",
      "0.7665\n",
      "0.8438\n",
      "0.7715\n",
      "0.7512\n",
      "0.8163\n",
      "0.7295\n",
      "0.7204\n",
      "0.7777\n",
      "0.7623\n",
      "0.7137\n",
      "0.7258\n",
      "0.7284\n",
      "0.6819\n",
      "0.7513\n",
      "0.6791\n",
      "0.7529\n",
      "0.7537\n",
      "0.6459\n",
      "0.7161\n",
      "0.7104\n",
      "0.7206\n",
      "0.6648\n",
      "0.7008\n",
      "0.7144\n",
      "0.7071\n",
      "0.6703\n",
      "0.7969\n",
      "0.6995\n",
      "0.6849\n",
      "0.6491\n",
      "0.6585\n",
      "0.6163\n",
      "0.7323\n",
      "0.6734\n",
      "0.6826\n",
      "0.7186\n",
      "0.6943\n",
      "0.6966\n",
      "0.7062\n",
      "0.6474\n",
      "0.6555\n",
      "0.6614\n",
      "0.7018\n",
      "0.6718\n",
      "0.7294\n",
      "0.6611\n",
      "0.6378\n",
      "0.6617\n",
      "0.6446\n",
      "0.6079\n",
      "0.642\n",
      "0.6485\n",
      "0.6517\n",
      "0.6152\n",
      "0.6194\n",
      "0.706\n",
      "0.6094\n",
      "0.6619\n",
      "0.6439\n",
      "0.6593\n",
      "0.6395\n",
      "0.6374\n",
      "0.5931\n",
      "0.6309\n",
      "0.614\n",
      "0.6303\n",
      "0.6197\n",
      "0.6912\n",
      "0.659\n",
      "0.6315\n",
      "0.6144\n",
      "0.6694\n",
      "0.6643\n",
      "0.6514\n",
      "0.7025\n",
      "0.6387\n",
      "0.6401\n",
      "0.6561\n",
      "0.6387\n",
      "0.6094\n",
      "0.6603\n",
      "0.6408\n",
      "0.6396\n",
      "0.6049\n",
      "0.5995\n",
      "0.6493\n",
      "0.6205\n",
      "0.628\n",
      "0.6828\n",
      "0.6436\n",
      "0.6441\n",
      "0.6775\n",
      "0.6771\n",
      "0.6473\n",
      "0.6683\n",
      "0.6177\n",
      "0.6811\n",
      "0.6318\n",
      "0.6288\n",
      "0.6725\n",
      "0.6505\n",
      "0.668\n",
      "0.6403\n",
      "0.6301\n",
      "0.6369\n",
      "0.6644\n",
      "0.6107\n",
      "0.6057\n",
      "0.6137\n",
      "0.6526\n",
      "0.6562\n",
      "0.6942\n",
      "0.5895\n",
      "0.6953\n",
      "0.6731\n",
      "0.6482\n",
      "0.648\n",
      "0.6457\n",
      "0.6514\n",
      "0.6872\n",
      "0.6546\n",
      "0.66\n",
      "0.5898\n",
      "0.6865\n",
      "0.6959\n",
      "0.5985\n",
      "0.6782\n",
      "0.6425\n",
      "0.6717\n",
      "0.6756\n",
      "0.6311\n",
      "0.6723\n",
      "0.664\n",
      "0.6183\n",
      "0.6423\n",
      "0.6464\n",
      "0.6866\n",
      "0.6912\n",
      "0.6177\n",
      "0.5864\n",
      "0.6425\n",
      "0.5719\n",
      "0.6965\n",
      "0.6226\n",
      "0.6367\n",
      "0.6598\n",
      "0.613\n",
      "0.6609\n",
      "0.6172\n",
      "0.6089\n",
      "0.6363\n",
      "0.5839\n",
      "0.6242\n",
      "0.6165\n",
      "0.6054\n",
      "0.6706\n",
      "0.6077\n",
      "0.6135\n",
      "0.6782\n",
      "0.631\n",
      "0.6548\n",
      "0.6247\n",
      "0.6495\n",
      "0.6413\n",
      "0.6499\n",
      "0.6156\n",
      "0.6385\n",
      "0.6397\n",
      "0.5998\n",
      "0.5967\n",
      "0.6457\n",
      "0.6218\n",
      "0.69\n",
      "0.6503\n",
      "0.5952\n",
      "0.6198\n",
      "0.6293\n",
      "0.6587\n",
      "0.7032\n",
      "0.6275\n",
      "0.6295\n",
      "0.6299\n",
      "0.6242\n",
      "0.6044\n",
      "0.6623\n",
      "0.6663\n",
      "0.6345\n",
      "0.6195\n",
      "0.6294\n",
      "0.6657\n",
      "0.6073\n",
      "0.6682\n",
      "0.5881\n",
      "0.6298\n",
      "0.6856\n",
      "0.6333\n",
      "0.6539\n",
      "0.6221\n",
      "0.703\n",
      "0.5914\n",
      "0.6408\n",
      "0.5693\n",
      "0.6055\n",
      "0.6571\n",
      "0.6206\n",
      "0.6687\n",
      "0.6095\n",
      "0.6007\n",
      "0.6419\n",
      "0.6201\n",
      "0.661\n",
      "0.6105\n",
      "0.6322\n",
      "0.6522\n",
      "0.6638\n",
      "0.6526\n",
      "0.6061\n",
      "0.6219\n",
      "0.6113\n",
      "0.6309\n",
      "0.6295\n",
      "0.6035\n",
      "0.6415\n",
      "0.6839\n",
      "0.6117\n",
      "0.6126\n",
      "0.671\n",
      "0.6168\n",
      "0.5879\n",
      "0.6392\n",
      "0.6505\n",
      "0.624\n",
      "0.5956\n",
      "0.6672\n",
      "0.6197\n",
      "0.6057\n",
      "0.628\n",
      "0.5817\n",
      "0.6486\n",
      "0.6219\n",
      "0.6487\n",
      "0.6027\n",
      "0.6418\n",
      "0.6598\n",
      "0.5862\n",
      "0.6061\n",
      "0.6473\n",
      "0.6489\n",
      "0.6412\n",
      "0.6108\n",
      "0.6225\n",
      "0.6622\n",
      "0.63\n",
      "0.6221\n",
      "0.5634\n",
      "0.5737\n",
      "0.627\n",
      "0.5913\n",
      "0.6254\n",
      "0.6307\n",
      "0.6374\n",
      "0.6208\n",
      "0.6193\n",
      "0.5984\n",
      "0.6303\n",
      "0.6507\n",
      "0.561\n",
      "0.5754\n",
      "0.6232\n",
      "0.6479\n",
      "0.6196\n",
      "0.6097\n",
      "0.6488\n",
      "0.6057\n",
      "0.6524\n",
      "0.6309\n",
      "0.6327\n",
      "0.5955\n",
      "0.6062\n",
      "0.5897\n",
      "0.619\n",
      "0.6205\n",
      "0.6506\n",
      "0.6458\n",
      "0.6393\n",
      "0.5626\n",
      "0.6307\n",
      "0.59\n",
      "0.6479\n",
      "0.6229\n",
      "0.6231\n",
      "0.6335\n",
      "0.6377\n",
      "0.6403\n",
      "0.6628\n",
      "0.5653\n",
      "0.6582\n",
      "0.6241\n",
      "0.6458\n",
      "0.626\n",
      "0.6226\n",
      "0.5973\n",
      "0.6612\n",
      "0.6419\n",
      "0.6272\n",
      "0.6599\n",
      "0.6305\n",
      "0.6401\n",
      "0.6313\n",
      "0.5676\n",
      "0.5986\n",
      "0.6057\n",
      "0.6011\n",
      "0.6405\n",
      "0.6392\n",
      "0.6374\n",
      "0.615\n",
      "0.5989\n",
      "0.6649\n",
      "0.6239\n",
      "0.5843\n",
      "0.6422\n",
      "0.6025\n",
      "0.5683\n",
      "0.6151\n",
      "0.6106\n",
      "0.6513\n",
      "0.5826\n",
      "0.6013\n",
      "0.6192\n",
      "0.5757\n",
      "0.5862\n",
      "0.6128\n",
      "0.5894\n",
      "0.5852\n",
      "0.5797\n",
      "0.5887\n",
      "0.6375\n",
      "0.5978\n",
      "0.623\n",
      "0.6164\n",
      "0.5666\n",
      "0.6258\n",
      "0.6184\n",
      "0.5839\n",
      "0.587\n",
      "0.576\n",
      "0.5932\n",
      "0.6497\n",
      "0.5592\n",
      "0.6603\n",
      "0.6082\n",
      "0.5734\n",
      "0.644\n",
      "0.625\n",
      "0.663\n",
      "0.6052\n",
      "0.6019\n",
      "0.5826\n",
      "0.6416\n",
      "0.6006\n",
      "0.6065\n",
      "0.6316\n",
      "0.6119\n",
      "0.6161\n",
      "0.6212\n",
      "0.6607\n",
      "0.6038\n",
      "0.6026\n",
      "0.5639\n",
      "0.6017\n",
      "0.5923\n",
      "0.6369\n",
      "0.5908\n",
      "0.5872\n",
      "0.6554\n",
      "0.5797\n",
      "0.6247\n",
      "0.5791\n",
      "0.5644\n",
      "0.6042\n",
      "0.602\n",
      "0.6431\n",
      "0.613\n",
      "0.5917\n",
      "0.6036\n",
      "0.612\n",
      "0.641\n",
      "0.623\n",
      "0.625\n",
      "0.6111\n",
      "0.5867\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m logs = trainer.state.log_history\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m logs:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m   \u001b[38;5;28mprint\u001b[39m(\u001b[43mi\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mloss\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[31mKeyError\u001b[39m: 'loss'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "logs = trainer.state.log_history\n",
    "for i in logs:\n",
    "  print(i[\"loss\"])\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
